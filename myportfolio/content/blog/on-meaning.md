---
title: "On Meaning and Mapping"
date: "2025-11-12"
excerpt: ""
tags: ["Philosophy", "LLMs", "Language"]
readTime: 7
---

Meaning is an avenue that I’m deeply drawn to, particularly because it is intangible. Communication is an attempt to express meaning.

Recently, I’ve realized that communication is an ever-iterating sequence of vector embeddings — an iteration of functions that map the intangible to the tangible.

When we speak to LLMs, words are mapped to tokens, which are mapped to vector embeddings. These words attempt to express some meaning, and we attempt to capture this meaning via continuous numbers. I believe the more non-discrete a mapping is, the more accurately it captures meaning. Words are plenty continuous with the massive lexicon of the English language, tokens even more continuous, but numbers are truly continuous. If we look at this micro-pipeline of words to numbers, LLMs succeed in their mapping.

Yet I am still unsatisfied with my interactions with LLMs. The LLM often provides output unsuited to my needs. I see this most clearly when learning abstract concepts. Recently, while developing an intuitive understanding of Lagrange duality, I felt an intangible hole in my understanding — something I couldn’t diagnose. Because I couldn’t articulate the gap, the LLM continuously gave answers that dodged what I needed. Only after some time, when I finally asked the *right* question, everything clicked.

Whose fault was this?  
The LLM’s job is to map words to embeddings and produce an output. Nothing in that internal pipeline malfunctioned. We have to zoom out. The failure occurred in my inability to translate the meaning I wished to convey into a discrete string of words.

In many ways, the English language itself is a vector embedding. I have an intangible meaning that must be mapped into the vector of English words — and this mapping is undeniably imperfect.

Look around us: translation, poetry, literature, public speaking, and most relevant today, prompt engineering — all exist within this grand pipeline of mapping accurate vector embeddings.

What do we do with this information? We can try to optimize LLMs. We treat LLMs as a standalone optimization problem. But if they are just one part of the larger pipeline, why not zoom out and optimize the *human* mapping — the function that converts internal meaning into the “vector embeddings” that LLMs receive?

One simple thought experiment lies in analyzing different languages.

Suppose **X** is the intangible meaning you wish to convey. Let **C** represent Chinese and **E** represent English. You can attempt to express **X** through either **C** or **E**. Their embeddings are unequal; therefore, one must be closer to **X**. This suggests that different languages convey meaning with different degrees of accuracy.

This leads to additional questions:  
Which language is the most accurate? What makes it more accurate?

And finally, the question that motivated this essay:

**What would it take to build a language that captures meaning itself?**
